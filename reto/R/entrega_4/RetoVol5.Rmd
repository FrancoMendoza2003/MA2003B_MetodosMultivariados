---
title: "RetoVol5"
author: "Angel Azahel Ramírez Cabello"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
data <- read.csv("Reto/classif/NORTE2_2020_2023_grouped.csv")
head(data)
#Checar si hay valores nulos
sum(is.na(data))
#Time series
plot(data$PM2.5,type="l")


```
```{r}
inicio=10000
final=inicio+168
```



Pruebas de hipótesis

$H_{0}$ = 0  Entonces la serie no es estacionaria

$H_{1}$ $\neq$ 0 Entonces la serie es estacionaria

```{r}
library(urca)
x=data[inicio:final,]
xts<-ts(x$PM2.5, start = c(1,1), frequency = 12)
#xts
head(x)
tail(x)
df = ur.df(xts, type = "none", lags = 0)
summary(df)
valorp=pf(df@testreg$fstatistic[1], df@testreg$fstatistic[2], df@testreg$fstatistic[3], lower.tail = FALSE)
#Condición para saber si valor p es menor a 0.05
valorp <- ifelse(valorp <= 0.05, 1, 0)
valorp
```




# Problema # 2

Para los datos de las ventas de televisores analiza la serie de tiempo (consúltalos en la diapositiva de Series de tiempo No Estacionarias):

```{r}
#ventas = c(4.8, 4.1, 6, 6.5, 5.8, 5.2, 6.8, 7.4, 6, 5.6, 7.5, 7.8, 6.3, 5.9, 8, 8.4)

ventas=data[inicio:final,]
x= ts(ventas$PM2.5, frequency = 12, start(c(1,1)))
plot.ts(x, col = "red", main="Ventas de televisores")
T = decompose(x,type = c("multiplicative"))
```
# Realiza el análisis de tendencia y estacionalidad

```{r}
plot(T, col="purple")
```

Como se puede ver en la gráfica de tendencia, claramente se sigue un patrón hacia valores cada vez más positivos después de cada trimestre.

Pruebas de hipótesis

$H_{0}$ = 0  Entonces la serie no es estacionaria

$H_{1}$ $\neq$ 0 Entonces la serie es estacionaria

```{r}
library(urca)
df = ur.df(x, type = "none", lags = 0)
summary(df)
```
 Como se obtiene un valor p bastante alto, la hipótesis nula no se puede rechazar y se concluye que existe estacionalidad.
 



# Analiza el modelo lineal de la tendencia:
```{r}
T$seasonal
```


# Realiza la regresión lineal de la tendencia (ventas desestacionalizadas vs tiempo)
```{r}
ventas_desestacionalizadas = (T$x)/(T$seasonal)
x3 = 1:169
y3 = x#ventas_desestacionalizadas
N3 = lm(y3~x3)
plot(x3, y3, type = "l")
abline(N3, col = "red")
text(6, 7, " ventas = 5.1080 + 0.1474 trimestre")
summary(N3)
```


```{r}
# Extract relevant data
ventas2 <- data[inicio:(final+168),]
x2 <- ts(ventas2$PM2.5, frequency = 12, start = c(1, 1))

# Predictions using N3
y4 <- predict(N3, data.frame(x3 = 170:181))
y5 <- predict(N3, data.frame(x3 = 182:193))
y6 <- predict(N3, data.frame(x3 = 194:205))
y7 <- predict(N3, data.frame(x3 = 206:217))
yr4 <- x2[1:217]

# Plotting
plot(x3, y3, type = "b", xlim = c(1, 217), ylim = c(5, 50), xlab = "Observation", ylab = "PM2.5 Levels", main = "PREDICCION CONCENTRACION PM2.5")
points(170:217, yr4[170:217], col = "red", pch = 16, cex = 0.8, lwd = 2, xlab = "Observation", ylab = "PM2.5 Levels", main = "ARIMA Model Prediction")
#points(182:217, c(y5, y6, y7), col = c("blue", "orange", "purple"), pch = 16, cex = 1.2)
points(170:181, y4, col = "green", pch = 16, cex = 0.8)
points(182:193, y5, col = "blue", pch = 16, cex = 0.8)
points(194:205, y6, col = "orange", pch = 16, cex = 0.8)
points(206:217, y7, col = "brown", pch = 16, cex = 0.8)
abline(N3, col = "purple", lwd = 2)
legend("topleft", legend = c("Observado", "Regresión lineal", "Próximas 48 horas"), col = c("black", "purple", "red"), pch = c(16, 16, 16))

# Displaying additional information
cat("Mean Squared Error (Next 12):", mean((yr4[182:193] - y5)^2), "\n")
cat("Mean Squared Error (Next 24):", mean((yr4[194:205] - y6)^2), "\n")
cat("Mean Squared Error (Next 36):", mean((yr4[206:217] - y7)^2), "\n")
cat("Mean of Observed (Next 12):", mean(yr4[182:193]), "\n")
cat("Mean of Forecasted (Next 12-14):", mean(c(y4, y5, y6)), "\n")
cat("Mean of Observed (Next 24):", mean(yr4[194:205]), "\n")
cat("Mean of Forecasted (Next 24-26):", mean(c(y5, y6, y7)), "\n")
cat("Mean of Observed (Next 36):", mean(yr4[206:217]), "\n")
cat("Mean of Forecasted (Next 36-38):", mean(c(y6, y7)), "\n")

#Datos
# Guardar en una lista el MSE de las predicciones
MSE <- c(mean((yr4[182:193] - y5)^2), mean((yr4[194:205] - y6)^2), mean((yr4[206:217] - y7)^2))
# Guardar en otra lista la diferencia absoluta  de medias entre predicciones y observaciones
DIF <- c(abs(mean(yr4[182:193]) - mean(c(y4, y5, y6))), abs(mean(yr4[194:205]) - mean(c(y5, y6, y7))), abs(mean(yr4[206:217]) - mean(c(y6, y7))))
print(DIF)
# Guardar en una lista si el promedio de cada observación es mayor a 30
PROM <- c(mean(yr4[170:181]) > 30,mean(yr4[182:193]) > 30, mean(yr4[194:205]) > 30, mean(yr4[206:217]) > 30)
PROM <- ifelse(PROM == TRUE, 1, 0)
# Guardar en una lista si el promedio de cada predicción es mayor a 30
PROM2 <- c(mean(y4) > 30, mean( y5)> 30,mean(y6) > 30, mean( y7)>30)
PROM2 <- ifelse(PROM2 == TRUE, 1, 0)
# Calcular las veces que PROM2 predice correctamente a PROM
accuracy=sum(PROM == PROM2)/length(PROM)
print(PROM)
print(PROM2)
print(accuracy)


```



# Analiza la pertinencia del modelo lineal:

El modelo completo presenta un valor p muy por debajo del nivel de significancia de 0.05, por lo que, es significativo para predecir las ventas trimestrales, por otro lado, cuenta con un valor de R^2 mayor a 90% lo que indica un nivel muy confiable para hacer predicciones

## Distribución Normal de los residuos:  

$H_{0}$ = 0  Entonces la distribución de los residuos de la regresión es normal

$H_{1}$ $\neq$ 0 Entonces la distribución de los residuos de la regresión no es parecida a la normal

```{r}
library (nortest) #Se utiliza la prueba Anderson - Darling porque es una muestra de más de 50 datos, lo cual se considera grande
shapiro.test(N3$residuals)
```
Por otro lado, el modelo cumple con una distribución normal en sus residuos, debido a que el valor p de la prueba indica un valor bastante alto a comparación de alpha, indicando así que es confiable. 

## Homocedasticidad (varianza constante) (librería lmtest)    

$H_{0}$ = 0  Entonces la varianza de los residuos es constante

$H_{1}$ $\neq$ 0 Entonces la varianza de los residuos no es constante y depende de los coeficientes

```{r}

#install.packages("lmtest")
library(lmtest)
bptest(N3)  

```

De acuerdo con los valor $p$ obtenido, el cual es mayor a $\alpha$, se rechaza $H_{0}$ y por ello se puede decir que la varianza es constante, lo cual indica que esta no depende de los coeficientes y esto se transmite en confianza para el modelo.

```{r}
plot(N3$fitted.values, N3$residuals)

abline(h=0, col = "red", lwd = 2)
```


## Independencia  (librería lmtest)

$H_{0}$ = 0  Los residuos son independientes entre sí

$H_{1}$ $\neq$ 0 Los residuos no son independientes entre sí y existe autocorrelación

```{r}

dwtest(N3) 
```
De acuerdo con el valor $p$ obtenido, el cual es menor a $\alpha$, se rechaza $H_{0}$, lo cual, quiere decir que los residuos no son independientes y tienen autocorrelación negativa, provocando que haya un sesgo, que probablemente ocurra por la naturaleza de serie de tiempo del modelo, como se puede ver en la siguiente gráfica:

```{r}
plot(N3 $residuals)

abline(h=0, col = "red", lwd = 2)
```
# Calcula el CME y el EPAM de la predicción de la serie de tiempo.

```{r}
predicciones <- predict(N3, newdata = data.frame(tiempo = x3))
cme <- mean((x - predicciones)^2)
epam <- mean(abs(x - predicciones)/x)

cat(" CME:",cme)
cat("\nEPAM: ",epam)


```

# Concluye sobre el modelo

Finalmente se encuentra que el modelo cumple casi todos los supuestos, lo cual lo vuelve altamente confiable para realizar predicciones y a su vez, cuenta con valores de errores relativamente bajos, mostrando así que el modelo es capaz de predecir periodos cercanos a la medición realizada.


# Realiza el pronóstico para el siguiente año.

```{r}
nuevo_tiempo <- data.frame(x3 = seq(max(x3) +1, max(x3) +4, by =1))
pronostico <- predict(N3, newdata = nuevo_tiempo)
cat("Pronóstico para los cuatro trimestres del siguiente año:\n")
pronostico
```